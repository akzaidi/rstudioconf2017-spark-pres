{
    "collab_server" : "",
    "contents" : "---\ntitle: \"What Spark Taught Me In the Past Year\"\nsubtitle: \"How I Learned to Stop Worrying and Enjoy  Being Lazy\"\nauthor: \"Ali_Zaidi %>% lightning_talk %>% rstudio::conf\"\ndate: \"2017/01/07\"\noutput:\n  xaringan::moon_reader:\n    lib_dir: libs\n    nature:\n      highlightStyle: github\n      highlightLines: true\n---\n\n```{r setup, include=FALSE}\noptions(htmltools.dir.version = FALSE)\nlibrary(knitr)\n```\n\n```{css}\n.callout-bold strong {\n  font-weight: normal;\n  color: #444444;\n  background-color: #FFFFAA;\n}\n.hljs-keyword {\n  font-weight: normal !important;\n}\nth {\n  text-align: left;\n}\ntd, th {\n  padding: 3px 20px 3px 0;\n}\n```\n\n\n## Apache Spark Core\n\n![](img/spark-core-apis.png)\n\n---\n\n## R and Spark Became Intimate Friends in 2016\n\n+ Last year was an exciting year for the R APIs for Spark:\n\n--\n\n1. [`SparkR`, 1.6 -> 2.0](https://spark.apache.org/docs/2.0.0/sparkr.html)\n  - Greater adoption of SparkML algorithms\n  - Introduction of user-defined functions/functionals\n      * `spark.lapply`, `dapply`, `gapply`, `gapply.collect`\n      * apply a user-defined function to each partition or group of a Spark DataFrame\n      * each group _must_ fit in the driver memory\n  - Issues: Doesn't play well with `dplyr`\n--\n\n2. [`sparklyr`](spark.rstudio.com)\n  - A complete _tidy_ interface to Spark\n  - Works seamlessly with `dplyr`\n  - IDE support to view Spark DataFrames and manage Spark connections\n  - Custom R UDFs are still in development ([#81](https://github.com/rstudio/sparklyr/issues/81))\n  - Awesome extension mechanism\n--\n\n3. [R Server on Spark](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-r-server-get-started)\n  - Fully managed Hadoop/Spark on the cloud\n  - Deploy `RevoScaleR` algorithms into Spark clusters\n      * Complete support for all the PEMA modeling functions in `RevoScaleR`\n---\n\n## Azure HDInsight\n### Full Managed Hadoop/Spark on the Cloud\n\n+ With Azure HDInsight, you don't have to choose between the APIs\n+ Can use any combination of the APIs for your data science application\n+ Focus less on optimizing code, rewriting your functions, and focus more on developing applications\n+ HDInsight R Server clusters ship with RStudio Server\n+ You can add additional nodes to scale your cluster out, shrink it down when you don't need the extra resources\n+ Run your own R functions across your cluster in paralle using `rxDataStep`, `rxExec` and `foreach`\n\n---\n\n## Understanding Spark as an R User\n\nCore components of Spark (left) and how to think about them as R programmers (right):\n--\n\n.pull-left[\n1. RDD;\n\n1. DataFrame;\n\n1. Transformers;\n  - a function that takes a RDD/DataFrame in and pushes another RDD/DataFrame out\n\n1. Actions;\n  - a function that takes a RDD/DataFrame in, and puts anything else out\n\n]\n--\n\n.pull-right[\n1. Distributed immutable `list`;\n\n1. Distributed `data.frame`;\n\n1. Lazy Computations;\n\n  <br>\n  <br>\n  <br>\n  <br>\n  \n1. Eager Queries;\n\n]\n---\n\nbackground-image: url(http://i.imgur.com/5GbW690.gif)\nbackground-size: 1200px\nbackground-position: 50% 50%\nclass: center, bottom, inverse\n\n\n# The Art of Being Lazy\n---\n\n## How To Use Spark Effectively with R\n\n\n1. Apache Spark's core computational framework is a functional programming paradigm for building DAGs (distributed acyclical graphs) for the data processing job at hand\n--\n\n  - DAGs contain the logical flow of how data is used to compute the result\n  - Spark optimizes computations within a Spark job by __caching__ data\n--\n\n1. Every job in Spark is split into:\n  - _stages_\n      * basic physical unit of operation\n--\n      * combination of as many transformations as possible into a single action, but no shuffles\n--\n      * unlike mapreduce, which only supports two-stages (map and reduce), any Spark job could have multiple stages that can run concurrently\n--\n  - _tasks_\n      * basic unit of operation per partition\n--\n1. When you need to reuse data across different Spark jobs, __cache__ the Spark DataFrame so you can reuse it\n--\n\n1. When you have data you have worked hard on tidying, save it to an efficient format, like _parquet_\n\n---\n\n## `sparklyr` \n### Tidy Data Manipulation in Spark\n\n* `sparklyr` makes common `tidyverse` operations very simple\n    - also, `spark_install` will save you from hair-loss!\n* Full support for `SparkML`\n\n\n```{r sparklyr, eval = FALSE, cache = FALSE}\n\norigins <- file.path(\"wasb://mrs-spark@alizaidi.blob.core.windows.net\",\n                     \"user/RevoShare/alizaidi/Freddie/Acquisition\")\nlibrary(sparklyr)\nsc <- spark_connect(\"yarn-client\")\nfreddie_origins <- spark_read_csv(sc,\n                                  path = origins,\n                                  name = 'freddie_origins',\n                                  header = FALSE,\n                                  delimiter = \"|\"\n                                  )\n\n```\n---\n\n## `sparklyr` \n### Tidy Data Manipulation in Spark\n\n```{r}\nfreddie_origins\n```\n\n---\n## `sparklyr` \n### Tidy Data Manipulation in Spark\n\n```{r}\nclass(freddie_origins)\n\n```\n\n--\n\n+ `tbl_spark` inherits `dplyr` methods from `tbl_sql`\n+ Spark SQL supports a subset of the SQL-92 language, so not all your SQL statements will work\n    - actually using the Hive Context, which supports a richer subset\n  \n---\n\n## `sparklyr` \n### Using `dplyr` with `tbl_spark`\n\n```{r rename}\nlibrary(dplyr)\nfreddie_rename <- freddie_origins %>% rename(\n                          credit_score = V1,\n                          first_payment = V2,\n                          first_home = V3,\n                          maturity = V4,\n                          msa = V5,\n                          mi_perc = V6,\n                          num_units = V7,\n                          occ_status = V8,\n                          cltv = V9,\n                          dti = V10,\n                          upb = V11,\n                          ltv = V12,\n                          orig_rate = V13,\n                          channel = V14,\n                          ppm = V15,\n                          prod_type = V16,\n                          state = V17,\n                          prop_type = V18,\n                          post_code = V19,\n                          loan_number = V20,\n                          loan_purpose = V21,\n                          orig_term = V22,\n                          num_borrowers = V23,\n                          seller = V24,\n                          servicer = V25\n                          )\nfreddie_rename %>% head\n```\n---\n\n## Create Date Fields\n\nThe origination date is buried inside the loan number field. We will pick it out by indexing the loan number substring:\n\n```{r select}\nfreddie_rename %>% select(loan_number)\n\n```\n---\n\n## Substring Operations\n\n```{r orig_year}\nfreddie_rename <- freddie_rename %>% \n  mutate(orig_date = substr(loan_number, 3, 4),\n         year = as.numeric(substr(loan_number, 3, 2)))\n\nfreddie <- freddie_rename %>% \n  mutate(orig_year = paste0(ifelse(year < 10, \"200\", \n                                   ifelse(year > 16, \"19\",\n                                          \"20\")), year))\n\nfreddie <- freddie %>% \n  mutate(orig_year = substr(orig_year, 1, 4))\n\nfreddie %>% head\n```\n---\n\n## Calculate Average Credit Score by Year\n\n\n```{r avg_fico}\n\nfico_year <- freddie %>% group_by(orig_year, state) %>% \n  summarise(ave_fico = mean(credit_score)) %>% collect\nfico_year %>% head\n\n```\n---\n\n## Summarize In a Function\n\n```{r avg_var}\n\nyear_state_sum <- function(val = \"credit_score\") {\n  \n  library(lazyeval)\n  \n  year_state <- freddie %>% group_by(orig_year, state) %>% \n    summarise_(sum_val = interp(~mean(var), var = as.name(val)))\n  \n  year_state <- year_state %>% collect\n  \n  names(year_state)[3] <- paste0(\"ave_\", val)\n  \n  return(year_state)\n  \n}\n\n```\n---\n\n## Plot\n\n```{r state_choro_dti, eval = FALSE}\n\nlibrary(rMaps)\nyear_state_sum(\"dti\") %>% \n  mutate(year = as.numeric(orig_year)) %>% \n  rMaps::ichoropleth(ave_dti ~ state, data = .,\n                     animate = \"year\",\n                     geographyConfig = list(popupTemplate = \"#!function(geo, data) {\n                                         return '<div class=\\\"hoverinfo\\\"><strong>'+\n                                         data.state + '<br>' + 'Average DTI in  '+ data.year + ': ' +\n                                         data.ave_dti.toFixed(2)+\n                                         '</strong></div>';}!#\")) -> state_map\n```\n---\n\nclass: inverse\n\n## Thanks!\n\n+ [spark.rstudio.com](spark.rstudio.com)\n+ [github.com/Azure/mr4ds](https://github.com/Azure/mr4ds)\n+ [Spark bookdown](https://bookdown.org/alizaidi/mrs-spark-ml/)",
    "created" : 1484316646584.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "383412197",
    "id" : "F4DF7415",
    "lastKnownWriteTime" : 1484336897,
    "last_content_update" : 1484336897691,
    "path" : "~/rstudioconf-spark-pres/1-spark-ninja.rmd",
    "project_path" : "1-spark-ninja.rmd",
    "properties" : {
        "last_setup_crc32" : "37C39299ff0891f9",
        "marks" : "<:79,2\n>:79,2"
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}