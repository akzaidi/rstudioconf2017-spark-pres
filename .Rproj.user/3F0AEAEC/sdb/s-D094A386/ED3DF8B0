{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Scalable Bayesian Data Analysis with Spark and Microsoft R Server\"\nauthor: '[Ali Zaidi, alizaidi@microsoft.com](mailto:alizaidi@microsoft.com)'\ndate: \"October 25th, 2016\"\noutput:\n  revealjs::revealjs_presentation:\n    center: yes\n    incremental: yes\n    previewLinks: yes\n    reveal_plugins:\n    - zoom\n    - notes\n    self_contained: no\n    slideNumber: yes\n    theme: night\n    viewDistance: 3\n---\n\n\n```{r global_options, include=FALSE}\nknitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.align = 'center', cache = TRUE)\n```\n\n\n# Learning with Large Bayesian Models\n\n## Unpacking the Title\n### Scalable Bayesian Data Analysis with Spark and Microsoft R Server\n\n+ Scalable - should be able to accommodate large datasets\n    - $$p > n$$\n    - streaming datasets\n    - generally large datasets where you want to do some inference\n+ R and Spark\n    - R is great at statistical modeling, visualization and inference\n    - R is also lazy, and functional\n    - Let's reuse the API\n+ Bayesian\n    - Like to be able to regularize automatically\n    - perform inference\n    \n## Microsoft R Server\n### WODA - Reusable API\n\n+ One of the core principles behind Microsoft R Server is the WODA framework\n    - WODA -- write once deploy anywhere\n    - Encourages API reuse\n+ Provides seamless transition from a local environment to a cluster environment\n+ Don't want to rewrite the entire code-base in Java/Scala\n+ Deployment should be possible within seconds\n\n## Microsoft R Server\n### Components\n\n![](img/mrs-diag.png)\n\n# R APIs for Spark - A Tale of Three APIs\n\n## The aRt of Being Lazy\n### Lazy Evaluation in R\n\n![](http://i.imgur.com/5GbW690.gif)\n\n\n* R, like it's inspiration, Scheme, is a _functional_ programming language\n* R evaluates lazily, delaying evaluation until necessary, which can make it very flexible\n* R has a high memory footprint, and can easily lead to crashes if you aren't careful\n\n## R APIs for Spark\n### `RxSpark`\n\n+ Allows the distribution of local R code into spark applications\n+ The only abstractions are those of the `RevoScaleR` package\n    - data resides as distributed on-disk objects (`xdfd`'s)\n    - Functions and data transformations are provided by traditional R objects\n+ Each `RevoScaleR` function invokes it's own Spark applications and converts XDFDs into RDDs/DataFrames\n    - application persists for the duration of the job to avoid JVM creation overhead\n    - for multi-iteration jobs, data is cached\n    - no need for the developer to write any Spark code\n+ Available through Azure HDInsight Clusters\n    - currently utilizing Spark 1.6\n+ User defined functions can be applied at the data partition level using `rxExec`\n\n## R APIs for Spark\n### `SparkR`\n\n+ The standard R API for Spark since 1.4 (MLLib support started in 1.5)\n+ R package provides functions that invoke functions directly on the JVM\n    - Uses a RPC server and provides JVM wrappers\n+ `DataFrame` support is inspired by the `dplyr` package\n    - tries to emulate `dplyr` syntax, but doesn't use NSE\n    - `dplyr`: `taxi %>% group_by(pickup_nhood) %>%` \n                `    summarise(ave_delay = mean(pickup_delay))`\n    - `SparkR`: `taxi %>% group_by(taxi$pickup_nhood) %>%`\n                `    summarise(ave_delay = mean(taxi$pickup_delay))`\n+ Limited ML: `glm`, `naive-bayes`, and `kmeans`\n+ Spark 2.0: support for custom UDFs using `dapply` and `gapply`\n    - each partition must fit into an R process on the worker node\n\n## R APIs for Spark\n### Masking with `dplyr`\n\n![](img/masking.png)\n\n## R APIs for Spark\n### `sparklyr`\n\n+ Turns out `dplyr` already has a SQL backend\n+ Since all Spark DataFrame operations are conducted at the Spark SQL level, utilize `dplyr`'s SQL backend rather than JVM wrappers\n+ 100% support for `dplyr` SQL inside of Spark, including NSE:\n    - `_lyr`: `taxi %>% group_by(pickup_nhood) %>%`\n                `   summarise(ave_delay = mean(pickup_delay))`\n+ Full support for all `SparkML`\n+ No UDF support yet\n\n## R APIs for Spark\n### `sparklyr` relies on RPC layer provided by `sparkapi`\n\n+ `sparklyr` is meant to be the DSL, providing easy data manipulation with `dplyr` and ML analogously to `stats` and other modeling packages \n+ The core RPC layer is not inside of `sparklyr`\n+ `sparkapi` provides the core R to Java RPC bridge publicly, and provides a simple extension mechanism to call arbitrary Spark APIs packages\n    - e.g., extensions to connect o [H20 Sparkling Water](http://spark.rstudio.com/h2o.html) with `sparkapi` now exist\n\n## Azure HDInsight\n### Full Managed Hadoop/Spark on the Cloud\n\n+ With Azure HDInsight, you don't have to choose\n+ Can use any combination of the APIs for your data science application\n+ Focus less on optimizing code, rewriting your functions, and focus more on developing applications\n\n# Data Manipulation Examples\n\n## `sparklyr`\n\n### Tidy Data Manipulation Using `dplyr` syntax\n\n+ For data scientists comfortable with R, using `sparklyr` requires no prior Spark knowledge\n+ `dplyr` statements are converted to SQL statements, sent to Catalyst and optimized\n\n## Import Into Spark DataFrames\n\n```{r sparklyr, cache = FALSE}\norigins <- file.path(\"wasb://mrs-spark@alizaidi.blob.core.windows.net\",\n                     \"user/RevoShare/alizaidi/Freddie/Acquisition\")\nlibrary(sparklyr)\nsc <- spark_connect(\"yarn-client\")\nfreddie_origins <- spark_read_csv(sc,\n                                  path = origins,\n                                  name = 'freddie_origins',\n                                  header = FALSE,\n                                  delimiter = \"|\"\n                                  )\n\n```\n\n## Using `dplyr`\n\n+ Now it's a Spark DataFrame\n+ It is also of class `tbl_sql`, so all `dplyr` methods are converted to `Spark SQL` statements and run on the spark application defined through the spark context `sc`\n\n```{r check_class}\nclass(freddie_origins)\nlibrary(dplyr)\nfreddie_origins %>% head\n\n```\n\n## Renaming Columns\n\n```{r rename}\nfreddie_rename <- freddie_origins %>% rename(\n                          credit_score = V1,\n                          first_payment = V2,\n                          first_home = V3,\n                          maturity = V4,\n                          msa = V5,\n                          mi_perc = V6,\n                          num_units = V7,\n                          occ_status = V8,\n                          cltv = V9,\n                          dti = V10,\n                          upb = V11,\n                          ltv = V12,\n                          orig_rate = V13,\n                          channel = V14,\n                          ppm = V15,\n                          prod_type = V16,\n                          state = V17,\n                          prop_type = V18,\n                          post_code = V19,\n                          loan_number = V20,\n                          loan_purpose = V21,\n                          orig_term = V22,\n                          num_borrowers = V23,\n                          seller = V24,\n                          servicer = V25\n                          )\nfreddie_rename %>% head\n```\n\n\n\n## Create Date Fields\n\nThe origination date is buried inside the loan number field. We will pick it out by indexing the loan number substring:\n\n```{r select}\nfreddie_rename %>% select(loan_number)\n\n```\n\n\n## Substring Operations\n\n```{r orig_year}\nfreddie_rename <- freddie_rename %>% \n  mutate(orig_date = substr(loan_number, 3, 4),\n         year = as.numeric(substr(loan_number, 3, 2)))\n\nfreddie <- freddie_rename %>% \n  mutate(orig_year = paste0(ifelse(year < 10, \"200\", \n                                   ifelse(year > 16, \"19\",\n                                          \"20\")), year))\n\nfreddie <- freddie %>% \n  mutate(orig_year = substr(orig_year, 1, 4))\n\nfreddie %>% head\n```\n\n\n## Calculate Average Credit Score by Year\n\n\n```{r avg_fico}\n\nfico_year <- freddie %>% group_by(orig_year, state) %>% \n  summarise(ave_fico = mean(credit_score)) %>% collect\nfico_year %>% head\n\n```\n\n## Summarize In a Function\n\n```{r avg_var}\n\nyear_state_sum <- function(val = \"credit_score\") {\n  \n  library(lazyeval)\n  \n  year_state <- freddie %>% group_by(orig_year, state) %>% \n    summarise_(sum_val = interp(~mean(var), var = as.name(val)))\n  \n  year_state <- year_state %>% collect\n  \n  names(year_state)[3] <- paste0(\"ave_\", val)\n  \n  return(year_state)\n  \n}\n\n```\n\n\n\n## Plot\n\n```{r state_choro_dti, eval = FALSE}\n\nlibrary(rMaps)\nyear_state_sum(\"dti\") %>% \n  mutate(year = as.numeric(orig_year)) %>% \n  rMaps::ichoropleth(ave_dti ~ state, data = .,\n                     animate = \"year\",\n                     geographyConfig = list(popupTemplate = \"#!function(geo, data) {\n                                         return '<div class=\\\"hoverinfo\\\"><strong>'+\n                                         data.state + '<br>' + 'Average DTI in  '+ data.year + ': ' +\n                                         data.ave_dti.toFixed(2)+\n                                         '</strong></div>';}!#\")) -> state_fico\n\nstate_fico$save(\"StateMapDTI.html\", cdn = T)\n\n```\n\n\n## Plot\n\n```{r, echo = FALSE}\nhtmltools::includeHTML(\"StateMapDTI.html\")\n\n```\n\n## Aggregate Performance Data\n### Getting Delinquencies from Historical Performance\n\n+ The performance data is similarly saved in Azure Blob Storage\n\n```{r perf_data, eval = FALSE}\n\nperf <- file.path(\"wasb://mrs-spark@alizaidi.blob.core.windows.net\",\n                     \"user/RevoShare/alizaidi/Freddie/Performance\")\nfreddie_perf <- spark_read_csv(sc,\n                                  path = perf,\n                                  name = 'freddie_perf',\n                                  header = FALSE,\n                                  delimiter = \"|\"\n)\n\n\n```\n\n## Aggregate Performance Data\n### Getting Delinquencies from Historical Performance\n\n```{r}\nfreddie_perf %>% select(V1, V4) %>% head\nfind_delinq_loans <- function(freddie_ts = freddie_perf) {\n  \n  #XX = Unknown\n  # • 0 = Current, or less\n  # than 30 days past due\n  # • 1 = 30-59 days\n  # delinquent\n  # • 2 = 60 – 89 days\n  # delinquent \n  # etc...\n  # R = REO Acquisition \n  \n  freddie_ts <- freddie_ts %>%\n    select(V1, V4) %>% filter(V4 != \"XX\")\n  freddie_ts <- freddie_ts %>% rename(loan_number = V1,\n                                      delq_status = V4)\n  \n  freddie_ts <- freddie_ts %>% \n    mutate(default = if (delq_status %REGEXP% \"[2-9*R]\") \"default\" else \"current\")\n  \n  return(freddie_ts)\n  \n  \n}\n\n```\n\n\n# Demo 1 - UDFs with `RxSpark`\n\n# Demo 2 - Bayesian Neural Networks",
    "created" : 1484331765124.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "917832973",
    "id" : "ED3DF8B0",
    "lastKnownWriteTime" : 1484331726,
    "last_content_update" : 1484331726,
    "path" : "~/Spark-Summit/scalable-bayes/scalable-bayes.Rmd",
    "project_path" : null,
    "properties" : {
        "marks" : "<:256,0\n>:257,-1"
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}